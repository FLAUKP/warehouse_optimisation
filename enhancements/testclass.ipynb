{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sqlalchemy import create_engine\r\n",
    "engine = create_engine('postgresql://postgres:BIDA123@localhost/testingopt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "\r\n",
    "#5, create worker list\r\n",
    "from faker import Faker\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "num_worker = 20\r\n",
    "\r\n",
    "faker = Faker()\r\n",
    "Faker.seed(4321)\r\n",
    "np.random.seed(1)\r\n",
    "\r\n",
    "df = []\r\n",
    "serial=1\r\n",
    "for n in range(20):\r\n",
    "    df.append({'worker_id': \"W\" + str(serial).zfill(3), 'name':faker.name()})\r\n",
    "    serial+=1\r\n",
    "\r\n",
    "df = pd.DataFrame(df)\r\n",
    "df = df[['worker_id','name']]\r\n",
    "df['salaryperhr']= np.random.randint(7,14,20) \r\n",
    "\r\n",
    "df1 = df[:num_worker]\r\n",
    "\r\n",
    "df1\r\n",
    " \r\n",
    "# insert one_time fixed table for all workers to postgresql\r\n",
    "df1.to_sql('dropzone_workers_all', engine, if_exists='replace',index=False)\\\r\n",
    " \r\n",
    "#df1.to_sql('dropzone_workers', engine, if_exists='replace',index=False) # insert table to postgresql\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df = pd.read_sql('SELECT * FROM dropzone_workers_all', con = engine)\r\n",
    "\r\n",
    "df.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   worker_id                   name  salaryperhr\n",
       "0       W001            Jason Brown           12\n",
       "1       W002            Jacob Stein           10\n",
       "2       W003             Cody Brown           11\n",
       "3       W004          Larry Morales            7\n",
       "4       W005      Jessica Hendricks            8\n",
       "5       W006            Brian Moore           10\n",
       "6       W007            Scott Baker           12\n",
       "7       W008           Ruth Hoffman            7\n",
       "8       W009          Daniel George            7\n",
       "9       W010            David Moody            8\n",
       "10      W011          Brian Maxwell           11\n",
       "11      W012            Cory Cooper           12\n",
       "12      W013          Morgan Foster           11\n",
       "13      W014         James Sandoval           13\n",
       "14      W015           Brian Lester            8\n",
       "15      W016             Anne Davis            9\n",
       "16      W017  Miss Amanda Harris MD           11\n",
       "17      W018               Amy Dunn           13\n",
       "18      W019              Brian Lee           12\n",
       "19      W020         Kimberly Pratt            9"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker_id</th>\n",
       "      <th>name</th>\n",
       "      <th>salaryperhr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W001</td>\n",
       "      <td>Jason Brown</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W002</td>\n",
       "      <td>Jacob Stein</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W003</td>\n",
       "      <td>Cody Brown</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W004</td>\n",
       "      <td>Larry Morales</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W005</td>\n",
       "      <td>Jessica Hendricks</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W006</td>\n",
       "      <td>Brian Moore</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>W007</td>\n",
       "      <td>Scott Baker</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>W008</td>\n",
       "      <td>Ruth Hoffman</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>W009</td>\n",
       "      <td>Daniel George</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W010</td>\n",
       "      <td>David Moody</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>W011</td>\n",
       "      <td>Brian Maxwell</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>W012</td>\n",
       "      <td>Cory Cooper</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>W013</td>\n",
       "      <td>Morgan Foster</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>W014</td>\n",
       "      <td>James Sandoval</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>W015</td>\n",
       "      <td>Brian Lester</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>W016</td>\n",
       "      <td>Anne Davis</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>W017</td>\n",
       "      <td>Miss Amanda Harris MD</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>W018</td>\n",
       "      <td>Amy Dunn</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>W019</td>\n",
       "      <td>Brian Lee</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>W020</td>\n",
       "      <td>Kimberly Pratt</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "class Summary:\r\n",
    "    def __init__(self,no_of_worker):\r\n",
    "        \r\n",
    "        df = pd.read_sql('SELECT * FROM dropzone_workers_all', con = engine)  \r\n",
    "        self.df = df.head(no_of_worker).to_sql(\"workers_\"+str(no_of_worker)+\"\",engine, if_exists='replace',index=False)\r\n",
    "        \r\n",
    "        skus = pd.read_sql('SELECT sku_id, standard_time FROM dropzone_sku', con = engine)\r\n",
    "        df2 = pd.read_sql('SELECT * FROM workers_'+str(no_of_worker)+'', con = engine) \r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "        sku_list = skus.values.tolist()\r\n",
    "        workers_list = df2['worker_id'].to_list()\r\n",
    "\r\n",
    "        productivity = []\r\n",
    "\r\n",
    "        np.random.seed(1)\r\n",
    "        for sku, mins in sku_list:\r\n",
    "            for worker in workers_list:\r\n",
    "                avg_time = int(np.random.normal(mins,3))\r\n",
    "                productivity.append({'sku_id': sku, 'worker': worker, 'avg_time': avg_time})\r\n",
    "\r\n",
    "        df = pd.DataFrame(productivity)\r\n",
    "        df.to_sql(\"dropzone_worker_\"+str(no_of_worker)+\"_sku_proficiency\", engine, if_exists='replace', index=False)\r\n",
    "        print(df)\r\n",
    "\r\n",
    "        \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    " Summary(10)\r\n",
    "\r\n",
    "\r\n",
    "# df5 = pd.DataFrame(f5)\r\n",
    "# print(df5)\r\n",
    "# df5.to_csv(\"12345679.csv\")\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      sku_id worker  avg_time\n",
      "0     AA0001   W001        41\n",
      "1     AA0001   W002        35\n",
      "2     AA0001   W003        35\n",
      "3     AA0001   W004        33\n",
      "4     AA0001   W005        39\n",
      "...      ...    ...       ...\n",
      "1995  AA0200   W006        35\n",
      "1996  AA0200   W007        36\n",
      "1997  AA0200   W008        32\n",
      "1998  AA0200   W009        36\n",
      "1999  AA0200   W010        39\n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.Summary at 0x2670bd2cb48>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from datetime import date, timedelta, datetime\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import csv\r\n",
    "import random\r\n",
    "import calendar\r\n",
    "from random import randrange\r\n",
    "\r\n",
    "\r\n",
    "class generate_date:\r\n",
    "    def __init__(self,year,month) :\r\n",
    "\r\n",
    "\r\n",
    "        self.sdate = datetime(year, month, 1, 0, 1, 22)   # start date\r\n",
    "        self.edate = datetime(year, month + 1, 1, 0, 1, 22) - timedelta(days=1)  # end date\r\n",
    "        date_modified = self.sdate\r\n",
    "        date_in_mth = [self.sdate.strftime(\"%d/%m/%Y %H:%M:%S\")]\r\n",
    "        #skus = pd.read_csv('dropzone_sku.csv')\r\n",
    "        skus = pd.read_sql('SELECT sku_id FROM dropzone_sku', con = engine)\r\n",
    "        ssku = set(skus['sku_id'])\r\n",
    "        month_name = calendar.month_name[month]\r\n",
    "        # print(self.sdate)\r\n",
    "        # print(self.edate)\r\n",
    "        while date_modified < self.edate:\r\n",
    "            date_modified += timedelta(days=1) \r\n",
    "            date_in_mth.append(date_modified.strftime(\"%d/%m/%Y %H:%M:%S\"))\r\n",
    "\r\n",
    "    # print(date_in_mth, '\\n')\r\n",
    "\r\n",
    "    # create orders for each day in month base on Poisson Distribution\r\n",
    "\r\n",
    "            nr_order_daily_list = [np.random.poisson(27) for i in range(30)]\r\n",
    "\r\n",
    "        # print(nr_order_daily_list, '\\n')\r\n",
    "\r\n",
    "\r\n",
    "            order_table = pd.DataFrame(columns = ['date', 'order_id', 'sku_id', 'qty'])\r\n",
    "        order_id = 1\r\n",
    "\r\n",
    "        for d, k in zip(date_in_mth, nr_order_daily_list): # k = total orders per day\r\n",
    "            #print(d, k)\r\n",
    "            for m in range(k):      # each loop will create list of SKUs per order\r\n",
    "                nr_sku_per_order = np.random.poisson(5)\r\n",
    "                order_sku_list = random.sample(ssku, nr_sku_per_order)\r\n",
    "                order_date = datetime.strptime(d, \"%d/%m/%Y %H:%M:%S\") + \\\r\n",
    "                timedelta(hours=randrange(24)) + timedelta(minutes=randrange(60))\r\n",
    "                for sku in order_sku_list:      # each loop will create qty (pcs) per sku \r\n",
    "                    nr_pcs_per_sku = int(np.random.normal(7,1))     # average 7 pcs per sku, std dev = 1\r\n",
    "                    order_table = order_table.append({'date': order_date, \\\r\n",
    "                    'order_id': 'SO'+str(order_id).zfill(5), 'sku_id': sku, 'qty': nr_pcs_per_sku}, \\\r\n",
    "                        ignore_index=True)\r\n",
    "                    order_id +=1\r\n",
    "\r\n",
    "        # print(order_table)\r\n",
    "        # print(month_name)\r\n",
    "        #order_table.to_sql(\"dropzone_order_\"+month_name+\"_\"+str(year)+\"\",engine,if_exists='replace',index=False)\r\n",
    "        \r\n",
    "\r\n",
    "        # Create batch_zone_order \r\n",
    "        #df = pd.read_sql(\"SELECT * FROM dropzone_order_\"+month_name+\"_\"+str(year)+\"\", con = engine)\r\n",
    "\r\n",
    "        order_table['date'] = pd.to_datetime(order_table['date'],format=\"%Y-%m-%d\")\r\n",
    "        order_table['date_hour'] = pd.DatetimeIndex(order_table['date']).hour\r\n",
    "\r\n",
    "\r\n",
    "        #create a list of condition for batching\r\n",
    "        roundingtime = [\r\n",
    "        (order_table['date_hour'] >= 0) & (order_table['date_hour'] < 8 ),\r\n",
    "        (order_table['date_hour'] >= 15) & (order_table['date_hour'] < 24 ),\r\n",
    "        (order_table['date_hour'] >= 8) & (order_table['date_hour'] < 10 ),\r\n",
    "        (order_table['date_hour'] >= 10 ) & (order_table['date_hour'] < 13 ),\r\n",
    "        (order_table['date_hour'] >= 13) & (order_table['date_hour'] < 15 )\r\n",
    "        ]\r\n",
    "\r\n",
    "        values =[8, 8, 10, 13, 15]\r\n",
    "         \r\n",
    "        order_table['timestamp'] = np.select(roundingtime,values)\r\n",
    "\r\n",
    "        #create a list of condition for next day \r\n",
    "\r\n",
    "        nextday = [\r\n",
    "        (order_table['date_hour'] >= 15) & (order_table['date_hour'] < 24 ),\r\n",
    "        (order_table['date_hour'] >= 0) & (order_table['date_hour'] < 15 )\r\n",
    "        ]\r\n",
    "\r\n",
    "        date = [order_table['date'].dt.date + timedelta(days=1),order_table['date'].dt.date]\r\n",
    "        order_table['date_1'] = np.select(nextday,date)\r\n",
    "\r\n",
    "\r\n",
    "        # combine date and time\r\n",
    "        order_table['batch_date'] = pd.to_datetime(order_table.date_1) + order_table.timestamp.astype('timedelta64[h]')\r\n",
    "        order_table['batch_date'] = pd.to_datetime(order_table['batch_date'], format='%Y-%m-%d %H:%M:%S')\r\n",
    "        order_table.sort_values(by=['batch_date'],inplace =True)\r\n",
    "        # order_table.reset_index(drop=True, inplace=True)\r\n",
    "        # order_table['task_id'] = 1 + order_table.index\r\n",
    "\r\n",
    "\r\n",
    "        order_table = order_table.drop(['timestamp','date_1','date_hour'], axis=1)\r\n",
    "        print(order_table)\r\n",
    "        #order_table.to_csv(\"dropzone_batch_order_table.csv\")\r\n",
    "        order_table.to_sql(\"dropzone_batch_order_\"+str(month)+'_'+str(year)+\"\",engine,if_exists='replace',index=False)\r\n",
    "\r\n",
    "        print(month_name, str(year)+ \" done\")\r\n",
    "\r\n",
    "        \r\n",
    "   \r\n",
    "    \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "generate_date(2021,6)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                    date order_id  sku_id qty          batch_date\n",
      "0    2021-06-01 02:27:22  SO00001  AA0152   6 2021-06-01 08:00:00\n",
      "70   2021-06-01 00:36:22  SO00071  AA0056   6 2021-06-01 08:00:00\n",
      "69   2021-06-01 00:36:22  SO00070  AA0167   7 2021-06-01 08:00:00\n",
      "68   2021-06-01 00:36:22  SO00069  AA0046   8 2021-06-01 08:00:00\n",
      "181  2021-06-01 03:25:22  SO00182  AA0013   7 2021-06-01 08:00:00\n",
      "...                  ...      ...     ...  ..                 ...\n",
      "3895 2021-06-30 20:17:22  SO03896  AA0193   7 2021-07-01 08:00:00\n",
      "3894 2021-06-30 20:17:22  SO03895  AA0046   5 2021-07-01 08:00:00\n",
      "3893 2021-06-30 20:17:22  SO03894  AA0172   7 2021-07-01 08:00:00\n",
      "3820 2021-06-30 22:54:22  SO03821  AA0122   7 2021-07-01 08:00:00\n",
      "3826 2021-06-30 22:54:22  SO03827  AA0150   7 2021-07-01 08:00:00\n",
      "\n",
      "[3958 rows x 5 columns]\n",
      "June 2021 done\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.generate_date at 0x2670b1910c8>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class sku_worker_matrix:\r\n",
    "    def __init__(self,no_of_worker,month,year) :\r\n",
    "\r\n",
    "        \r\n",
    "        # breakdown total month orders into daily batches \r\n",
    "        # create sku-worker matrix for optimisation assignment\r\n",
    "\r\n",
    "\r\n",
    "        #workers_df = pd.read_csv(\"dropzone_worker_sku_proficiency.csv\") \r\n",
    "        self.workers_df = pd.read_sql(\"SELECT * from dropzone_worker_\"+str(no_of_worker)+\"_sku_proficiency\", con = engine)\r\n",
    "        #print(self.workers_df.head(), '\\n')\r\n",
    "\r\n",
    "        self.batch_df = pd.read_sql('SELECT date, batch_date, sku_id, qty from dropzone_batch_order_'+str(month)+'_'+str(year)+'', con = engine)\r\n",
    "        #print(self.batch_df.head(), '\\n')\r\n",
    "\r\n",
    "        self.batch_df['date_2'] = pd.DatetimeIndex(self.batch_df['batch_date']).dayofweek\r\n",
    "        a=len(self.batch_df)\r\n",
    "        self.batch_start_date = datetime(year, month, 1, 8, 0, 0)   # start date\r\n",
    "        self.batch_end_date = datetime(year, month, 2 , 8, 0, 0)   # end date\r\n",
    "        print(a)\r\n",
    "        print(self.batch_start_date)\r\n",
    "        print(self.batch_end_date)\r\n",
    "        print(self.batch_start_date.strftime(\"%Y-%m-%d\"))\r\n",
    "        b = 0\r\n",
    "        c = 0 \r\n",
    "        d = 0\r\n",
    "\r\n",
    "        while b < a:  \r\n",
    "            batch1 = self.batch_df.loc[(self.batch_df['batch_date'] >= self.batch_start_date) & (self.batch_df['batch_date'] < self.batch_end_date) ]\r\n",
    "            #print(batch1)\r\n",
    "        \r\n",
    "        \r\n",
    "            if (batch1['date_2'].iloc[-1]) == 1 or (batch1['date_2'].iloc[-1]) == 2 or (batch1['date_2'].iloc[-1]) == 3 or (batch1['date_2'].iloc[-1]) == 0  :\r\n",
    "            \r\n",
    "\r\n",
    "                #if (batch1['date_2'].iloc[-1]) == 1\r\n",
    "\r\n",
    "                batch1agg = batch1[['sku_id', 'qty']].groupby(['sku_id'], sort=True).sum()\r\n",
    "                print(batch1, '\\n')\r\n",
    "                print(batch1['date_2'].iloc[-1])\r\n",
    "   \r\n",
    "                productivity1 = self.workers_df.join(batch1agg, on='sku_id', how='left')  # .set_index('sku_id')\r\n",
    "                print(productivity1, '\\n')\r\n",
    "\r\n",
    "                # productivity1['avg_time_add'] = productivity1['avg_time'] + productivity1['qty'] * 1\r\n",
    "                workers_productivity1 = productivity1.pivot(index='worker', columns='sku_id', values='avg_time_add')\r\n",
    "                self.batch_start_date += timedelta(days=1)\r\n",
    "                self.batch_end_date += timedelta(days=1)\r\n",
    "                #print(workers_productivity1)\r\n",
    "                # print(batch_start_date.weekday())\r\n",
    "                print(self.batch_start_date)\r\n",
    "                c =len(batch1)\r\n",
    "                #workers_productivity1.to_csv(\"dropzone_workers_productivity\"+str(self.batch_start_date.strftime(\"%Y-%m-%d\"))+\".csv\")\r\n",
    "                print(c)\r\n",
    "                b += c\r\n",
    "                d += 1\r\n",
    "                print(b)\r\n",
    "                print('yes'+str(d))\r\n",
    "            #print(workers_productivity1, '\\n')\r\n",
    "\r\n",
    "    \r\n",
    "            elif (batch1['date_2'].iloc[-1]) == 4 :\r\n",
    "\r\n",
    "     \r\n",
    "                batch1agg = batch1[['sku_id', 'qty']].groupby(['sku_id'], sort=True).sum()\r\n",
    "                print(batch1, '\\n')\r\n",
    "                print(batch1['date_2'].iloc[-1])\r\n",
    "            \r\n",
    "                productivity1 = self.workers_df.join(batch1agg, on='sku_id', how='left')  # .set_index('sku_id')\r\n",
    "                #print(productivity1.head(), '\\n')\r\n",
    "\r\n",
    "                productivity1['avg_time_add'] = productivity1['avg_time'] + productivity1['qty'] * 1\r\n",
    "                #workers_productivity1 = productivity1.pivot(index='worker', columns='sku_id', values='avg_time_add')\r\n",
    "                self.batch_end_date += timedelta(days=3)\r\n",
    "                self.batch_start_date += timedelta(days=1)\r\n",
    "                #print(workers_productivity1)\r\n",
    "                # print(batch_start_date.weekday())\r\n",
    "                print(self.batch_start_date)\r\n",
    "                c =len(batch1)\r\n",
    "                print(c)\r\n",
    "                b += c\r\n",
    "                print(b)\r\n",
    "                #workers_productivity1.to_csv(\"dropzone_workers_productivity\"+str(self.batch_start_date.strftime(\"%Y-%m-%d\"))+\".csv\")\r\n",
    "                d += 1\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "            # elif (batch1['date_2'].iloc[0]) == 5:\r\n",
    "\r\n",
    "            #     batch1agg = batch1[['sku_id', 'qty']].groupby(['sku_id'], sort=True).sum()\r\n",
    "            #     print(batch1, '\\n')\r\n",
    "            #     print(batch1['date_2'].iloc[-1])\r\n",
    "            \r\n",
    "            #     productivity1 = self.workers_df.join(batch1agg, on='sku_id', how='left')  # .set_index('sku_id')\r\n",
    "            #     #print(productivity1.head(), '\\n')\r\n",
    "\r\n",
    "            #     productivity1['avg_time_add'] = productivity1['avg_time'] + productivity1['qty'] * 1\r\n",
    "            #     #workers_productivity1 = productivity1.pivot(index='worker', columns='sku_id', values='avg_time_add')\r\n",
    "            #     self.batch_end_date += datetime.timedelta(days=1)\r\n",
    "            #     self.batch_start_date += datetime.timedelta(days=3)\r\n",
    "            #     #print(workers_productivity1)\r\n",
    "            #     # print(batch_start_date.weekday())\r\n",
    "            #     print(self.batch_start_date)\r\n",
    "            #     c =len(batch1)\r\n",
    "            #     print(c)\r\n",
    "            #     b += c\r\n",
    "            #     print(b)\r\n",
    "            #     #workers_productivity1.to_csv(\"dropzone_workers_productivity\"+str(self.batch_start_date.strftime(\"%Y-%m-%d\"))+\".csv\")\r\n",
    "            #     d += 1\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "sku_worker_matrix(5,6,2021)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3958\n",
      "2021-06-01 08:00:00\n",
      "2021-06-02 08:00:00\n",
      "2021-06-01\n",
      "                   date          batch_date  sku_id  qty  date_2\n",
      "0   2021-06-01 02:27:22 2021-06-01 08:00:00  AA0152    6       1\n",
      "1   2021-06-01 00:36:22 2021-06-01 08:00:00  AA0056    6       1\n",
      "2   2021-06-01 00:36:22 2021-06-01 08:00:00  AA0167    7       1\n",
      "3   2021-06-01 00:36:22 2021-06-01 08:00:00  AA0046    8       1\n",
      "4   2021-06-01 03:25:22 2021-06-01 08:00:00  AA0013    7       1\n",
      "..                  ...                 ...     ...  ...     ...\n",
      "106 2021-06-01 13:20:22 2021-06-01 15:00:00  AA0198    6       1\n",
      "107 2021-06-01 13:20:22 2021-06-01 15:00:00  AA0069    6       1\n",
      "108 2021-06-01 13:20:22 2021-06-01 15:00:00  AA0199    5       1\n",
      "109 2021-06-01 13:20:22 2021-06-01 15:00:00  AA0143    6       1\n",
      "110 2021-06-01 13:20:22 2021-06-01 15:00:00  AA0047    7       1\n",
      "\n",
      "[111 rows x 5 columns] \n",
      "\n",
      "1\n",
      "     sku_id worker  avg_time  qty\n",
      "0    AA0001   W001        41  NaN\n",
      "1    AA0001   W002        35  NaN\n",
      "2    AA0001   W003        35  NaN\n",
      "3    AA0001   W004        33  NaN\n",
      "4    AA0001   W005        39  NaN\n",
      "..      ...    ...       ...  ...\n",
      "995  AA0200   W001        34  NaN\n",
      "996  AA0200   W002        28  NaN\n",
      "997  AA0200   W003        34  NaN\n",
      "998  AA0200   W004        36  NaN\n",
      "999  AA0200   W005        34  NaN\n",
      "\n",
      "[1000 rows x 4 columns] \n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'avg_time_add'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'avg_time_add'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b9e022fec750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msku_worker_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-f7b2e32724ad>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, no_of_worker, month, year)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;31m# productivity1['avg_time_add'] = productivity1['avg_time'] + productivity1['qty'] * 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mworkers_productivity1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproductivity1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'worker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sku_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg_time_add'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_start_date\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_end_date\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   7786\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7788\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7790\u001b[0m     _shared_docs[\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    514\u001b[0m             )\n\u001b[0;32m    515\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[0mindexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_listlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3453\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3454\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3455\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'avg_time_add'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('train': conda)"
  },
  "interpreter": {
   "hash": "302b1d483689c65a1e50ba80157e0e67ffb79525a1b7cef62fdd87116227ac6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}